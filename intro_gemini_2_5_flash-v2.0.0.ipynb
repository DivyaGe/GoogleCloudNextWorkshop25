{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqi5B7V_Rjim"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyPmicX9RlZX"
   },
   "source": [
    "# Intro to Gemini 2.5 Flash\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MqT58L6Rm_q"
   },
   "source": [
    "| Authors |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |\n",
    "| [Holt Skinner](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVxnv1D5RoZw"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
    "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
    "</a>\n",
    "\n",
    "With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Flash can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy.\n",
    "\n",
    "Gemini 2.5 Flash is:\n",
    "\n",
    "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality\n",
    "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
    "- An amazing model for code, with particularly strong web development\n",
    "- Particularly good for complex prompts, while still being well rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfFPCBL4Hq8x"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Flash model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Generate text from text prompts\n",
    "  - Generate streaming text\n",
    "- Configure thinking budget\n",
    "- Start multi-turn chats\n",
    "- Use asynchronous methods\n",
    "- Configure model parameters\n",
    "- Set system instructions\n",
    "- Use safety filters\n",
    "- Use controlled generation\n",
    "- Count tokens\n",
    "- Process multimodal (audio, code, documents, images, video) data\n",
    "- Use automatic and manual function calling\n",
    "- Code execution\n",
    "- Thinking mode examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPiTOAHURvTM"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHRZUpfWSEpp"
   },
   "source": [
    "### Install Google Gen AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sG3_LKsWSD3A",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart the kernel after libraries are loaded\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a34b28cb8d5a"
   },
   "source": [
    "### Set Google Cloud project information and create client\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: peaceful-doodad-462404-c2\n",
      "Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.auth\n",
    "\n",
    "# Get project information from the system\n",
    "try:\n",
    "    credentials, PROJECT_ID = google.auth.default()\n",
    "    if PROJECT_ID is None:\n",
    "        # Fallback to environment variable if google.auth.default() doesn't provide it\n",
    "        PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\") or os.environ.get(\"GCP_PROJECT\") or os.environ.get(\"CLOUD_PROJECT\")\n",
    "        if PROJECT_ID is None:\n",
    "            raise ValueError(\"Could not determine Google Cloud Project ID from default credentials or environment variables.\")\n",
    "except Exception as e:\n",
    "    # If there's an error getting it dynamically, you might want to hardcode or prompt the user.\n",
    "    # For now, we'll set a placeholder and print an error.\n",
    "    print(f\"Warning: Could not automatically determine PROJECT_ID. Using placeholder. Error: {e}\")\n",
    "    PROJECT_ID = \"your-fallback-project-id\" # Fallback if dynamic retrieval fails\n",
    "\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"} # Keep this as a parameter for now\n",
    "\n",
    "# Create the Gemini API client\n",
    "from google import genai\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Print the project ID and location\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "\n",
    "\n",
    "# Create the Gemini API client\n",
    "from google import genai\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "from google.genai.types import (\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    GoogleSearch,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    ThinkingConfig,\n",
    "    Tool,\n",
    "    ToolCodeExecution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4yRkFg6BBu4"
   },
   "source": [
    "## Use the Gemini 2.5 Flash model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "### Load the Gemini 2.5 Flash model\n",
    "\n",
    "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash-preview-04-17\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "### Generate text from text prompts\n",
    "\n",
    "Use the `generate_content()` method to generate responses to your prompts.\n",
    "\n",
    "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xRJuHj0KZ8xz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem step-by-step:\n",
       "\n",
       "1.  **Find out how many tennis balls are in the new cans:** Roger buys 2 cans, and each can has 3 balls.\n",
       "    2 cans * 3 balls/can = 6 tennis balls\n",
       "\n",
       "2.  **Add the new balls to the balls he already had:** Roger started with 5 balls and bought 6 more.\n",
       "    5 balls + 6 balls = 11 tennis balls\n",
       "\n",
       "Roger has 11 tennis balls now."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lLIxqS6_-l8"
   },
   "source": [
    "### Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZiwWBhXsAMnv",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to calculate the total number of punches:\n",
       "\n",
       "1.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Calculate the total fight time:**\n",
       "    *   Each round is 3 minutes.\n",
       "    *   There are 5 rounds.\n",
       "    *   Total time = 3 minutes/round * 5 rounds = 15 minutes\n",
       "\n",
       "2.  **Calculate the total punches:**\n",
       "    *   Joe throws "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "25 punches per minute.\n",
       "    *   The fight lasted 15 minutes.\n",
       "    *   Total punches = 25 punches/minute * 15 minutes = 375 punches\n",
       "\n",
       "Joe threw a total of **375** punches."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"On average Joe throws 25 punches per minute. A fight lasts 5 rounds of 3 minutes. How many punches did he throw?\",\n",
    "):\n",
    "    display(Markdown(chunk.text))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EeLlqi2e5RI"
   },
   "source": [
    "## Thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBtGmbvaD9Mk"
   },
   "source": [
    "### Configure thinking budget\n",
    "\n",
    "The thinking budget allows the model to dynamically think for a task or select how many tokens to use for reasoning for certain tasks. It allows users to control quality and speed of response. Setting budget to `0` turns off thinking and turns the model into a non-thinking model for simpler tasks.\n",
    "\n",
    "You set the optional `thinking_budget` parameter in the `ThinkingConfig` to control and configure how much a model thinks on a given user prompt.\n",
    "\n",
    "- When unset -> dynamic thinking (default)\n",
    "- When set to  `0` -> thinking is disabled.  \n",
    "- When set to `[1-24576]` ->  model uses the allocated thinking budget\n",
    "\n",
    "Then use the `generate_content` or `generate_content_stream` method to send a request to generate content with the `thinking_config`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6LT7dm2FDTo3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down the practical implications of the P vs. NP problem for algorithm design and cryptography.\n",
       "\n",
       "First, a quick recap of the terms:\n",
       "\n",
       "*   **P (Polynomial time):** The class of decision problems (yes/no questions) for which a solution can be *found* quickly by a deterministic computer. \"Quickly\" means in a time that grows polynomially with the size of the input (e.g., n, n², n³ steps). These are problems considered \"tractable\" or \"easy to solve\".\n",
       "*   **NP (Nondeterministic Polynomial time):** The class of decision problems for which a *proposed solution* can be *verified* quickly by a deterministic computer. Finding the solution itself might be hard, but checking if a given answer is correct is easy. Many important practical problems fall into NP.\n",
       "*   **NP-Complete:** The \"hardest\" problems in NP. They are in NP, and any other problem in NP can be transformed into an NP-Complete problem quickly. If you find a fast (polynomial-time) algorithm for *any* NP-Complete problem, you've found a fast algorithm for *all* problems in NP.\n",
       "*   **P vs. NP:** The question is whether P = NP or P ≠ NP.\n",
       "    *   **P = NP:** Every problem whose solution can be quickly *verified* can also be quickly *found*.\n",
       "    *   **P ≠ NP:** There are problems whose solutions can be quickly *verified*, but which cannot be quickly *found* (specifically, NP problems that are not in P).\n",
       "\n",
       "Virtually all computer scientists believe that **P ≠ NP**.\n",
       "\n",
       "Now, let's look at the implications based on the two possible outcomes:\n",
       "\n",
       "**Implications for Algorithm Design:**\n",
       "\n",
       "1.  **If P = NP (Highly Unlikely):**\n",
       "    *   **Revolutionary Breakthrough:** This would be the most significant algorithmic discovery in history.\n",
       "    *   **Solving Currently Intractable Problems:** Many fundamental problems that we currently consider extremely hard to solve optimally (like the Traveling Salesperson Problem, Boolean Satisfiability (SAT), Graph Coloring, Integer Linear Programming, optimal scheduling, protein folding, drug discovery optimization, etc.) would suddenly become solvable quickly.\n",
       "    *   **Shift in Algorithm Design:** Algorithm design for these problems would change from finding clever approximation algorithms, heuristics, or exponential-time brute-force variants (only feasible for small inputs) to finding and implementing the newfound polynomial-time optimal algorithms.\n",
       "    *   **Massive Economic and Scientific Impact:** Optimizing logistics, financial portfolios, manufacturing processes, biological designs, and much more could be done precisely and efficiently. AI systems could solve complex planning and decision problems optimally.\n",
       "\n",
       "2.  **If P ≠ NP (Widely Believed & Current Reality):**\n",
       "    *   **Confirmation of Current Limitations:** This confirms our current experience: for many important problems in NP, there is no general, efficient algorithm that can always find the optimal solution quickly for all possible inputs.\n",
       "    *   **Justification for Current Approaches:** It justifies the ongoing research and use of:\n",
       "        *   **Approximation Algorithms:** Algorithms that run quickly and find solutions that are guaranteed to be close to optimal (e.g., within a certain percentage).\n",
       "        *   **Heuristics:** Algorithms and techniques that use rules of thumb to find good solutions quickly, though without guarantees of optimality or closeness to optimality.\n",
       "        *   **Exponential-Time Algorithms:** Algorithms like branch-and-bound that explore the solution space exhaustively but might be acceptable for small input sizes or specific problem instances.\n",
       "        *   **Algorithms for Specific Cases:** Developing polynomial-time algorithms for restricted versions of NP-hard problems (e.g., TSP on specific types of graphs).\n",
       "    *   **Focus on Managing Complexity:** Algorithm design continues to focus on managing the inherent complexity of hard problems, finding practical trade-offs between optimality, speed, and resource usage. It's about finding *good enough* solutions efficiently rather than guaranteed optimal solutions.\n",
       "    *   **The \"Hardness Barrier\":** It establishes a theoretical \"hardness barrier\" for certain types of problems, guiding researchers away from searching for a general polynomial-time algorithm for them.\n",
       "\n",
       "**Implications for Cryptography:**\n",
       "\n",
       "Cryptography relies heavily on the assumption that certain problems are **hard** to solve quickly. These \"hard\" problems are often in NP, or at least are reducible to problems believed not to be in P.\n",
       "\n",
       "1.  **If P = NP (Highly Unlikely):**\n",
       "    *   **Catastrophic Failure of Current Cryptography:** Most, if not all, modern public-key cryptography systems would be completely broken. These systems (like RSA, Diffie-Hellman, Elliptic Curve Cryptography) rely on the difficulty of problems like factoring large numbers, computing discrete logarithms, or finding short vectors in lattices – problems believed to be hard (not in P). If P=NP, algorithms would exist to solve these problems quickly.\n",
       "    *   **Impossibility of Secure Digital Communication/Transactions (using current methods):** Secure online banking, e-commerce, confidential communication, digital signatures, and secure authentication would become impossible using current cryptographic protocols. Anyone could potentially decrypt messages, forge signatures, or impersonate others.\n",
       "    *   **Need for a Paradigm Shift:** Cryptography would need entirely new foundations, perhaps relying on physical security assumptions or properties of the universe rather than computational hardness. This would be a crisis for global digital infrastructure.\n",
       "\n",
       "2.  **If P ≠ NP (Widely Believed & Current Reality):**\n",
       "    *   **Foundation for Modern Cryptography:** This is the crucial assumption that makes modern cryptography possible. The belief that P ≠ NP (specifically, that certain problems like factoring or discrete log are hard) provides the necessary computational \"asymmetry\" between sender and receiver, or between legitimate users and attackers.\n",
       "    *   **Security Based on Hardness:** The security of systems like RSA relies on the *practical impossibility* for an attacker to factor a large number quickly, even though verifying if a claimed factor is correct is easy (multiplication). This fits the NP model – verification is easy, finding the solution (factors) is hard.\n",
       "    *   **Justification for Research into Hard Problems:** Cryptography research continues to look for and utilize problems believed to be hard (not in P) to build secure systems. It also investigates potential weaknesses (e.g., quantum computing algorithms like Shor's algorithm, which *would* break current crypto even if P ≠ NP, by providing a polynomial-time solution for factoring and discrete log on a *quantum* computer).\n",
       "\n",
       "**In Summary:**\n",
       "\n",
       "*   **P vs. NP is fundamentally about the existence of efficient algorithms for problems where checking a solution is easy.**\n",
       "*   **If P = NP:** It would revolutionize algorithm design, making currently intractable optimization problems easy to solve, but simultaneously destroy most modern cryptography, leading to a global security crisis.\n",
       "*   **If P ≠ NP (the accepted belief):** It validates our current approach in algorithm design (using approximations, heuristics, etc., for hard problems) and provides the essential foundation for the security of modern cryptography, confirming that many problems we rely on for security are indeed computationally hard to break.\n",
       "\n",
       "The P vs. NP question isn't just a theoretical curiosity; its resolution has profound, practical implications for what computers can efficiently do and how we can secure digital information. The widespread belief that P ≠ NP shapes both fields significantly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What are the practical implications of the P vs. NP problem for algorithm design and cryptography?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=ThinkingConfig(\n",
    "            thinking_budget=THINKING_BUDGET,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzXrgiUTzsPw"
   },
   "source": [
    "Optionally, you can print the `usage_metadata` and token counts from the model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xOiHH_vCymZ_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_tokens_details=None cached_content_token_count=None candidates_token_count=1420 candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1420)] prompt_token_count=18 prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=18)] thoughts_token_count=798 tool_use_prompt_token_count=None tool_use_prompt_tokens_details=None total_token_count=2236 traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
      "798\n",
      "2236\n"
     ]
    }
   ],
   "source": [
    "print(response.usage_metadata)\n",
    "print(response.usage_metadata.thoughts_token_count)\n",
    "print(response.usage_metadata.total_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkYQATRxAK1_"
   },
   "source": [
    "#### Example prompts\n",
    "\n",
    "- What are the potential advantages and limitations of applying federated learning to train models on sensitive financial data?\n",
    "- What are the challenges and benefits of using transformer models (like BERT or GPT) for protein structure prediction compared to traditional methods?\n",
    "- (Try your own prompts!)\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b1443a27a08"
   },
   "source": [
    "For the following examples, we will set the thinking budget to `0` to reduce latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef63b60e2cb6"
   },
   "outputs": [],
   "source": [
    "thinking_config = ThinkingConfig(thinking_budget=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "### Start a multi-turn chat\n",
    "\n",
    "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
    "\n",
    "The context of the conversation is preserved between messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbM12JaLWjiF"
   },
   "outputs": [],
   "source": [
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQem1halYDBW"
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUJR4Pno-LGK"
   },
   "source": [
    "This follow-up prompt shows how the model responds based on the previous prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Fn69TurZ9DB"
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "### Send asynchronous requests\n",
    "\n",
    "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSReaLazs-dP"
   },
   "outputs": [],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
    "\n",
    "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
    "\n",
    "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9NXP5N2Pmfo"
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=2.0,\n",
    "        top_p=0.95,\n",
    "        candidate_count=1,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instructions\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A-yANiyCLaO"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to Spanish.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
    "\n",
    "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
    "\n",
    "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPlDRaloU59b"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"Be as mean and hateful as possible. Use profanity\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        safety_settings=safety_settings,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Response will be `None` if it is blocked.\n",
    "print(response.text)\n",
    "# Finish Reason will be `SAFETY` if it is blocked.\n",
    "print(response.candidates[0].finish_reason)\n",
    "# Safety Ratings show the levels for each filter.\n",
    "for safety_rating in response.candidates[0].safety_ratings:\n",
    "    print(safety_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "Gemini is a multimodal model that supports multimodal prompts.\n",
    "\n",
    "You can include any of the following data types from various sources.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Data type</th>\n",
    "      <th>Source(s)</th>\n",
    "      <th>MIME Type(s)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Text</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code> <code>text/html</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Document</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>application/pdf</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Image</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Audio</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td>\n",
    "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
    "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
    "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
    "        <code>audio/wav</code> <code>audio/webm</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Video</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
    "      <td>\n",
    "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
    "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
    "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4npg1tNTYB9"
   },
   "source": [
    "### Send local image\n",
    "\n",
    "Download an image to local storage from Google Cloud Storage.\n",
    "\n",
    "For this example, we'll use this image of a meal.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4avkv0Z7qUI-"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umhZ61lrSyJh"
   },
   "outputs": [],
   "source": [
    "with open(\"meal.png\", \"rb\") as f:\n",
    "    image = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6UxrUWef8RY"
   },
   "source": [
    "### Send document from Google Cloud Storage\n",
    "\n",
    "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
    "\n",
    "Check out this notebook for more examples of document understanding with Gemini:\n",
    "\n",
    "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItsE0aIuf-Wt"
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
    "            mime_type=\"application/pdf\",\n",
    "        ),\n",
    "        \"Summarize the document.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx-oidWIhPIi"
   },
   "source": [
    "### Send audio from General URL\n",
    "\n",
    "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSQWtO_jhPag"
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
    "            mime_type=\"audio/mpeg\",\n",
    "        ),\n",
    "        \"Write a summary of this podcast episode.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        audio_timestamp=True,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D3_oNUTuW2q"
   },
   "source": [
    "### Send video from YouTube URL\n",
    "\n",
    "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7-w8G_2wAOw"
   },
   "outputs": [],
   "source": [
    "video = Part.from_uri(\n",
    "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        video,\n",
    "        \"At what point in the video is Harry Potter shown?\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a189458e35c1"
   },
   "source": [
    "### Send web page\n",
    "\n",
    "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).\n",
    "\n",
    "**NOTE:** The URL must be publicly accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "661df0bdb542"
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs/\",\n",
    "            mime_type=\"text/html\",\n",
    "        ),\n",
    "        \"Write a summary of this documentation.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
    "\n",
    "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
    "\n",
    "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjSgf2cDN_bG"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeyDWbnxO-on"
   },
   "outputs": [],
   "source": [
    "parsed_response: Recipe = response.parsed\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7duWOq3vMmS"
   },
   "outputs": [],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response_dict = response.parsed\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
    "\n",
    "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhNElguLRRNK"
   },
   "outputs": [],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BsP0vXOY7hg"
   },
   "source": [
    "## Search as a tool (Grounding)\n",
    "\n",
    "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
    "\n",
    "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
    "\n",
    "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
    "\n",
    "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_M_4RRBdO_3"
   },
   "source": [
    "### Google Search\n",
    "\n",
    "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
    "\n",
    "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeR09J3AZT4U"
   },
   "outputs": [],
   "source": [
    "google_search_tool = Tool(google_search=GoogleSearch())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the current temperature in Austin, TX?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[google_search_tool],\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))\n",
    "\n",
    "print(response.candidates[0].grounding_metadata)\n",
    "\n",
    "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
    "\n",
    "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
    "\n",
    "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSUWWlrrlR-D"
   },
   "source": [
    "### Python Function (Automatic Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRR8HZhLlR-E"
   },
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Example method. Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    weather_map: dict[str, str] = {\n",
    "        \"Boston, MA\": \"snowing\",\n",
    "        \"San Francisco, CA\": \"foggy\",\n",
    "        \"Seattle, WA\": \"raining\",\n",
    "        \"Austin, TX\": \"hot\",\n",
    "        \"Chicago, IL\": \"windy\",\n",
    "    }\n",
    "    return weather_map.get(location, \"unknown\")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the weather like in San Francisco?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[get_current_weather],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4syyLEClGcn"
   },
   "source": [
    "### OpenAPI Specification (Manual Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BDQPwgcxRN3"
   },
   "outputs": [],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.function_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhDs2X3o0neK"
   },
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
    "\n",
    "The Gemini API provides code execution as a tool, similar to function calling.\n",
    "After you add code execution as a tool, the model decides when to use it.\n",
    "\n",
    "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W-3c7sy0nyz"
   },
   "outputs": [],
   "source": [
    "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "## Code\n",
    "\n",
    "```py\n",
    "{response.executable_code}\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "{response.code_execution_result}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5b5adb2eb70"
   },
   "source": [
    "## Thinking model examples\n",
    "\n",
    "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
    "\n",
    "### **Example 1:** Code generation\n",
    "\n",
    "Gemini 2.5 Flash excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
    "\n",
    "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f120dff0d16"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML.\n",
    "  I like pixelated dinosaurs and interesting backgrounds.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecf22b47bdc3"
   },
   "source": [
    "### **Example 2:** Multimodal reasoning (Geometry)\n",
    "\n",
    "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60260c0ac118"
   },
   "outputs": [],
   "source": [
    "image_file_url = (\n",
    "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
    ")\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c972334f62ff"
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"What's the area of the overlapping region?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52656e92cd69"
   },
   "source": [
    "### **Example 3**:  Math and problem solving\n",
    "\n",
    "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d46387bdc9e6"
   },
   "outputs": [],
   "source": [
    "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46b694793eb0"
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"How do I use three of the pool balls to sum up to 30?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "## What's next\n",
    "\n",
    "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rZV2TY5Pa3Dd",
    "mSUWWlrrlR-D",
    "h4syyLEClGcn"
   ],
   "name": "intro_gemini_2_5_flash.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
